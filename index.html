<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <title>Luke Logan</title>
    <meta name="viewport" content="width=device-width,initial-scale=1.0,shrink-to-fit=no">
    <link rel="stylesheet" href="assets/css/style.css" />
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  </head>

  <body>

    <h1>Luke Logan</h1>

    <nav class="navbar">
      <ul>
        <li class="navbar-active">
          <a href="">
            Home
          </a>
        </li>

        <li>
          <a href="resume">
            Resume
          </a>
        </li>

        <li>
          <a href="https://github.com/lukemartinlogan/">
            Github
          </a>
        </li>

        <li>
          <a href="https://scholar.google.com/citations?hl=en&user=Ljn7CmYAAAAJ">
            Google Scholar
          </a>
        </li>

        <li>
          <a href="https://www.linkedin.com/in/lukemartinlogan">
            LinkedIn
          </a>
        </li>
      </ul>
    </nav>

    <article id="Background" class="overview">
      <h2>Background</h2>

      <div class="overview-body">
        <div class="overview-text">
          <p>
            I am an HPC software engineer and PhD student with the Gnosis
            Research Center at Illinois Tech. My research aims to make
            distributed storage and operating systems more programmable,
            portable, and performant. HPC and machine learning applications
            perform massive data movements, putting extreme stress on storage
            systems to satisfy a wide variety of conflicting I/O and
            computational objectives. However, with rapidly evolving storage
            hardware and software, the design of these systems need to be
            reconsidered.
          </p>

          <p>
            Other interests of mine are algorithm design and data science, which are both skills
            that are useful to parallel/distributed processing research. I have been involved in
            numerous projects surrounding parallel processing, algorithm design, and data science.
            Projects I have worked on are listed below.
          </p>
        </div>

        <figure class="overview-img">
          <img src="assets/img/my-picture.jpg" alt="An image of Luke Logan"/>
          <figcaption>Me</figcaption>
        </figure>
      </div>
    </article>

    <article id="Projects">
      <h2>Systems Research Projects</h2>

      <nav class="project-nav">
        <ul>
          <li>
            <a href="http://www.cs.iit.edu/~scs/assets/projects/Hermes/Hermes.html">
              <h3>Hermes</h3>
              <figure>
                <img src="assets/img/Hermes/Hermes.png" alt="Hermes diagram"/>
                <figcaption> Hermes diagram. </figcaption>
              </figure>
              <p>
                Large-scale applications such as simulation, deep learning,
                and data analytics produce and analyze massive volumes of
                data. These data movements are typically the main bottleneck
                of these applications. To reduce the gap between computation
                and storage, HPC and Cloud sites have integrated fast tiers
                of storage, known as the Deep Memory and Storage Hierarchy (DMSH).
                However, efficiently utilizing the DMSH is complex, especially
                when heterogeneity is considered. Hermes is a new I/O buffering
                platform that places, shuffles, and migrates data within the
                DMSH automatically. Hermes is a $4 Million dollar NSF project,
                where I was the lead engineer and primarily responsible for the
                implementation, testing / debugging, and evaluation of Hermes.
              </p>
            </a>
          </li>

          <li>
            <a href="https://github.com/scs-lab/labstor">
              <h3>LabStor</h3>
              <figure>
                <img src="assets/img/LabStor/LabStorComponents.png" alt="LabStor diagram"/>
                <figcaption> LabStor diagram. </figcaption>
              </figure>
              <p>
                Traditionally, I/O systems have been developed within the
                confines of a centralized OS kernel. This led to monolithic and
                rigid storage systems that are limited by low development speed,
                expressiveness, and performance. However, this monolithic design
                philosophy makes it difficult to develop and deploy new I/O
                approaches to satisfy the rapidly-evolving I/O requirements of
                modern scientific applications. To this end, we propose LabStor:
                a modular and extensible platform for developing
                high-performance, customized I/O stacks. Evaluations show that
                by switching to a fully modular design, tailored I/O stacks can
                yield performance improvements of up to 60% in various
                applications.
              </p>
            </a>
          </li>

          <li>
            <a href="https://ieeexplore.ieee.org/abstract/document/9555982">
              <h3>pMEMCPY</h3>
              <figure>
                <img src="assets/img/pMEMCPY/pMEMCPY.png" alt="pMEMCPY diagram"/>
                <figcaption> pMEMCPY diagram. </figcaption>
              </figure>
              <p>
                Persistent memory (PMEM) devices can achieve comparable
                performance to DRAM while providing significantly more capacity.
                This has made the technology compelling as an expansion to main
                memory. Rethinking PMEM as storage devices can offer a high
                performance buffering layer for HPC applications to temporarily,
                but safely store data. However, modern parallel I/O libraries,
                such as HDF5 and pNetCDF, are complicated and introduce
                significant software and metadata overheads when persisting data
                to these storage devices, wasting much of their potential. In
                this work, we explore the potential of PMEM as storage through
                pMEMCPY: a simple, lightweight, and portable I/O library for
                storing data in persistent memory. We demonstrate that our
                approach is up to 2x faster than other popular parallel I/O
                libraries under real workloads.
              </p>
            </a>
          </li>

          <li>
            <a href="https://ieeexplore.ieee.org/abstract/document/9556010">
              <h3>HFlow</h3>
              <figure>
                <img src="assets/img/hflow/HFlow.png" alt="HFlow diagram"/>
                <figcaption> HFlow diagram. </figcaption>
              </figure>
              <p>
                To mask the I/O gap, new system architectures include
                intermediate I/O resources (e.g., Burst Buffers). However, the
                management of these resources lead to a number of concerns. They
                are managed in isolation, leading to inefficient interactions.
                Each technology manages a static set of resources, leading to
                load imbalance. Jobs receive exclusive access to resources,
                leading to resource under-utilization. Lastly, they have limited
                support for in-transit computations, which can reduce data
                movement cost. We present HFlow: a data forwarding framework
                that adopts a continuous data movement approach. Results show
                that HFlow can boost application I/O performance by up to 3x.
              </p>
            </a>
          </li>

          <li>
            <a href="https://dl.acm.org/doi/abs/10.1145/3431379.3460640">
              <h3>Apollo</h3>
              <figure>
                <img src="assets/img/apollo/apollo.png" alt="Apollo diagram"/>
                <figcaption> Apollo diagram. </figcaption>
              </figure>
              <p>
                Middleware services require low-latency access to telemetry data
                in order to make optimal decisions. However, typical monitoring
                services store this data in databases, resulting in significant
                latency penalties. This work presents Apollo: a low-latency
                monitoring service that aims to provide applications and
                middleware libraries with low latency access to relational
                telemetry data. Apollo provides this using a Pub-Sub API and
                machine learning to predict changes in the data to avoid
                constant polling. Evaluations showcase that Apollo can achieve
                sub-millisecond latency for acquiring complex insights with a
                memory overhead of 57MB and CPU overhead being only 7% more than
                existing state-of-the-art systems.
              </p>
            </a>
          </li>

          <li>
            <a href="https://ieeexplore.ieee.org/abstract/document/9139838">
              <h3>HCompress</h3>
              <figure>
                <img src="assets/img/hcompress/HCompress2.png" alt="HCompress diagram"/>
                <figcaption> HCompress diagram. </figcaption>
              </figure>
              <p>
                Modern scientific applications are bounded by data access speeds. HCompress is a C++
                library for caching data in a heterogeneous distributed environment. It employs a
                dynamic and adaptive compression system for matching compression libraries to
                different storage devices using reinforcement learning. We found that HCompress
                improved the performance of real-world workloads by as much as 700% relative to
                other modern storage solutions.
              </p>
            </a>
          </li>
        </ul>
      </nav>


      <article id="OtherProjects">
        <h2>Other Research Projects</h2>

        <nav class="project-nav">
          <ul>
            <li>
              <a href="https://ieeexplore.ieee.org/abstract/document/9101391">
                <h3>BOSSA</h3>
                <figure>
                  <img src="assets/img/bossa/quadrants.png" alt="BOSSA Platform figure" />
                  <figcaption> BOSSA Platform </figcaption>
                </figure>
                <p>
                  The FCC estimated that 10,000 lives could be saved annually if commercial mobile
                  radio service (CMRS) providers were able to find the indoor location of emergency
                  callers in a multi-story building. The BOSSA Platform is a proof-of-concept system
                  that can find the indoor location of a cell phone using signals from Bluetooth Low
                  Energy (BLE) beacons and a backend server for calculating indoor location based off
                  of signal strengths. In our trials, we were able to estimate the floor the caller
                  was on 100% of the time as well as the position on the floor to within 10 meters of
                  accuracy.
                </p>
              </a>
            </li>

            <li>
              <a href="projects/PotholeDetectives">
                <h3>Pothole Detectives (PhDs)</h3>
                <figure>
                  <img src="assets/img/potholes/route.png" alt="Example pothole filling route"/>
                  <figcaption> Pothole route </figcaption>
                </figure>
                <p>
                  Pothole damage costs US drivers $3 billion per year. In order to combat this
                  problem, driverless pothole-filling robots will become reality. In this project, we
                  developed a routing algorithm based off of a combination of the Chinese Postman
                  Problem and the Traveling Salesman Problem. We used OpenStreetMap and Geopandas to
                  obtain graphs of the wards of Chicago and used the Chicago Data Portal to get the
                  set of active pothole reports in Chicago.
                </p>
              </a>
            </li>
          </ul>
        </nav>
    </article>

    <footer class="navbar">
      <ul>
        <li>
          <a href="https://github.com/lukemartinlogan/">
            Github
          </a>
        </li>

        <li>
          <a href="https://scholar.google.com/citations?hl=en&user=Ljn7CmYAAAAJ">
            Google Scholar
          </a>
        </li>

        <li>
          <a href="https://www.linkedin.com/in/lukemartinlogan">
            LinkedIn
          </a>
        </li>
      </ul>
    </footer>

  </body>
</html>
